{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b8da9a",
   "metadata": {
    "id": "70b8da9a"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "import os, re, string, gzip, itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a291608c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a291608c",
    "outputId": "75f071b3-be54-4fef-b09a-aba8ebe6d7b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /Users/satyammishra/opt/anaconda3/lib/python3.8/site-packages (1.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a1af82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42a1af82",
    "outputId": "60d4257a-632f-4da0-e0a1-6bbcc338897c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cb9e6808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC News_LSTM.ipynb.html            classfication.ipynb\r\n",
      "\u001b[1m\u001b[36mPOC \u001b[m\u001b[m/                               \u001b[1m\u001b[36miTerm.app\u001b[m\u001b[m/\r\n",
      "bert_uncased_L-12_H-768_A-12_1.tar\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5db7c88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>number_of_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxic</td>\n",
       "      <td>15294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxic</td>\n",
       "      <td>1595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>8449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>7877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category  number_of_comments\n",
       "0          toxic               15294\n",
       "1   severe_toxic                1595\n",
       "2        obscene                8449\n",
       "3         threat                 478\n",
       "4         insult                7877\n",
       "5  identity_hate                1405"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"toxic_comments.csv\")\n",
    "df_toxic = df.drop(['id', 'comment_text'], axis=1)\n",
    "counts = []\n",
    "categories = list(df_toxic.columns.values)\n",
    "for i in categories:\n",
    "    counts.append((i, df_toxic[i].sum()))\n",
    "df_stats = pd.DataFrame(counts, columns=['category', 'number_of_comments'])\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ba2b8fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106912,)\n",
      "(52659,)\n"
     ]
    }
   ],
   "source": [
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
    "X_train = train.comment_text\n",
    "X_test = test.comment_text\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca921a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "497bc579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing Features\n",
      "['from vmbkubvmsdccbuffaloedu neil b gandler subject need diode model for pspice organization university at buffalo lines newssoftware vaxvms vnews nntppostinghost ubvmsdccbuffaloedu im designing a circuit with just a silicon diode i dont need to modify any of the parameters but the problem will not accept the following statement model diode d the pspice book i have is terrible i would appreciate any help neil gandler gandler electronics home automation electronic design technology'\n",
      " 'from rick miller former spook subject alternate legal wiretaps organization just me lines nntppostinghost summary nothing spooky its an executive order tuinstrasignalececlarksonedusoe writes it would be a strong incentive as vesselin points out for more police agencies to go rogue and try to get keys through more efficient but less constitutional means notice what the release said q suppose a law enforcement agency is conducting a wiretap on a drug smuggling ring and intercepts a conversation encrypted using the device what would they have to do to decipher the message a they would have to obtain legal authorization normally a court order to do the wiretap in the first place the clear implication is that there are legal authorizations other than a court order just how leaky are these and who knows whats in those pages that authorized the nsa there i was a cryptologic tech in the us navy ctrsn nothing big all spooks in the navy are required to know the gist of ussid the navyway of naming a particular presidential executive order it outlines what spooks can and cant do with respect to the privacy of us nationals the following information is of course unclassified the whole issue hangs about what you mean by wiretap if the signal can be detected by nonintrusive means like radio listening then it may be recorded and it may be analyzed analyzed means that it may be either deciphered andor radiolocation may be used to locate the transmitter the catch is this any and all record of the signal and its derivatives may only be kept for a maximum of days after which they are destroyed unless permission is obtained from the us attorney general to keep them didnt you ever wonder how coast guard cutters find those drugrunners in all those tens of thousands of square miles of sea even in the dark rick miller ricxjo muelisto send a postcard get one back enposxtigu bildkarton kaj vi ricevos alion rick miller woods muskego wis usa'\n",
      " 'from mathew subject re koresh is god organization mantis consultants cambridge uk xnewsreader rusnews v lines the latest news seems to be that koresh will give himself up once hes finished writing a sequel to the bible mathew'\n",
      " 'from mandtbackafinaboabofi mats andtbacka subject re an anecdote about islam inreplyto jaegerbuphybuedus message of apr gmt organization unorganized usenet postings uninc xnewsreader vms news lines in jaegerbuphybuedu writes deletia i dont understand the point of this petty sarcasm it is a basic principle of islam that if one is born muslim or one says i testify that there is no god but god and mohammad is a prophet of god that so long as one does not explicitly reject islam by word then one must be considered muslim by all muslims so the phenomenon youre attempting to make into a general rule or psychology is a direct odds with basic islamic principles if you want to attack islam you could do better than than to argue against something that islam explicitly contradicts in the deletions somewhere it mentioned something about chopping off of hands being a punishment for theft in saudi arabia assuming this is so i wouldnt know and assuming it is done by people fitting your requirement for muslim which i find highly likely then would you please try to convince bobby mozumder that muslims chop peoples hands off come back when youve succeeded disclaimer its great to be young and insane'\n",
      " 'from nrpstandrewsacuk norman r paterson subject islam vs the jehovahs witnesses organization society for trying really hard lines in article qpliuprincetonedu writes in article jbrownbatmanbmdtrwcom writes but god created lucifer with a perfect nature and gave him along with the other angels free moral will now god could have prevented lucifers fall by taking away his ability to choose between moral alternatives worship god or worship himself so lucifers moral choices are determined by his will what determines what his will is qpliuprincetonedu standard opinion opinions are deltacorrelated bobby a few posts ago you said that lucifer had no free will from the above it seems the jw believes the contrary are you talking about the same lucifer if so can you suggest an experiment to determine which of you is wrong or do you claim that you are both right norman']\n",
      "Test accuracy is 0.0002655337227827934\n",
      "... Processing 0\n",
      "[0 0 1 0 1]\n",
      "Test accuracy is 0.9778279341476368\n",
      "... Processing 1\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9703929899097186\n",
      "... Processing 2\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9710568242166755\n",
      "... Processing 3\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9714551248008497\n",
      "... Processing 4\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9804832713754646\n",
      "... Processing 5\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9754381306425917\n",
      "... Processing 6\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9835369091874668\n",
      "... Processing 7\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9787573021773766\n",
      "... Processing 8\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9904407859798194\n",
      "... Processing 9\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9843335103558152\n",
      "... Processing 10\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9907063197026023\n",
      "... Processing 11\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9881837493361657\n",
      "... Processing 12\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9687997875730218\n",
      "... Processing 13\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9743759957514604\n",
      "... Processing 14\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9868560807222517\n",
      "... Processing 15\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9795539033457249\n",
      "... Processing 16\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.977562400424854\n",
      "... Processing 17\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9820764737121614\n",
      "... Processing 18\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.9768985661178969\n",
      "... Processing 19\n",
      "[0 0 0 0 0]\n",
      "Test accuracy is 0.977562400424854\n"
     ]
    }
   ],
   "source": [
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model using X_dtm & y\n",
    "    SVC_pipeline.fit(X_train, train[category])\n",
    "    # compute the testing accuracy\n",
    "    print( SVC_pipeline.predict(X_test[:5]))\n",
    "    prediction = SVC_pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a4dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4201ebfd",
   "metadata": {
    "id": "4201ebfd"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8393509",
   "metadata": {
    "id": "a8393509"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy_score(clf, train_data, train_target, test_data, test_target, gs = True):\n",
    "    clf_ = clf.fit(train_data, train_target)\n",
    "    predicted_test_target = clf_.predict(test_data)\n",
    "    predicted_train_target = clf_.predict(train_data)\n",
    "    if True:\n",
    "        print('For {} classifier accuracy on train data is {}'.format(clf_.steps[-1][-1], accuracy_score(train_target,predicted_train_target)))\n",
    "        print('For {} classifier accuracy on test data is {}'.format(clf_.steps[-1][-1], accuracy_score(test_target, predicted_test_target)))\n",
    "    else:\n",
    "        print('For {} classifier accuracy on train data is {}'.format(clf_, accuracy_score(train_target,predicted_train_target)))\n",
    "        print('For {} classifier accuracy on test data is {}'.format(clf_, accuracy_score(test_target, predicted_test_target)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fA79UZEAtilQ",
   "metadata": {
    "id": "fA79UZEAtilQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(pattern=r'<.*?>', repl=' ', string=text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    text=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = text.strip()\n",
    "  \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6d16150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanText(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e4e87ee3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4e87ee3",
    "outputId": "2bb33cf4-11c4-41a6-879a-9693bf7ef2ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/satyammishra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words =set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "69553332",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "             'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b0a72ead",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0a72ead",
    "outputId": "102226da-1fa0-41f8-d73a-12cd918ce9e1"
   },
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "   categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "   categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0b22f252",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b22f252",
    "outputId": "2189b9d9-eb83-4704-84c0-4574d54838ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "398235b6",
   "metadata": {
    "id": "398235b6"
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42))])\n",
    "text_clf_stop = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', MultinomialNB())])\n",
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "            ])\n",
    "\n",
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                    fit_prior=True, class_prior=None))),\n",
    "            ])\n",
    "\n",
    "\n",
    "classfiers = [text_clf, text_clf_svm, text_clf_stop, SVC_pipeline, LogReg_pipeline, NB_pipeline]\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf-svm__alpha': (1e-2, 1e-3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1c6dc5e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "1c6dc5e1",
    "outputId": "d253a97b-acc4-46fa-8e96-d6433f2795cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: sd345@city.ac.uk (Michael Collier)\\nSubj...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSu...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: s0612596@let.rug.nl (M.M. Zwart)\\nSubjec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: stanly@grok11.columbiasc.ncr.com (stanly...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Features  Target\n",
       "0  From: sd345@city.ac.uk (Michael Collier)\\nSubj...       1\n",
       "1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\...       1\n",
       "2  From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSu...       3\n",
       "3  From: s0612596@let.rug.nl (M.M. Zwart)\\nSubjec...       3\n",
       "4  From: stanly@grok11.columbiasc.ncr.com (stanly...       3"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = dict(zip(twenty_train.data , twenty_train.target))\n",
    "train= pd.DataFrame.from_dict(my_dict, orient= 'index')\n",
    "train.reset_index(inplace=True)\n",
    "train.columns = ['Features', 'Target']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1746fe43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "1746fe43",
    "outputId": "6487a489-07f2-45eb-b2a6-c5df563b86e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: brian@ucsd.edu (Brian Kantor)\\nSubject: ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: rind@enterprise.bih.harvard.edu (David R...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: adwright@iastate.edu ()\\nSubject: Re: ce...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: livesey@solntze.wpd.sgi.com (Jon Livesey...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jhpb@sarto.budd-lake.nj.us (Joseph H. Bu...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Features  Target\n",
       "0  From: brian@ucsd.edu (Brian Kantor)\\nSubject: ...       2\n",
       "1  From: rind@enterprise.bih.harvard.edu (David R...       2\n",
       "2  From: adwright@iastate.edu ()\\nSubject: Re: ce...       2\n",
       "3  From: livesey@solntze.wpd.sgi.com (Jon Livesey...       0\n",
       "4  From: jhpb@sarto.budd-lake.nj.us (Joseph H. Bu...       3"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = dict(zip(twenty_test.data , twenty_test.target))\n",
    "test= pd.DataFrame.from_dict(my_dict, orient= 'index')\n",
    "test.reset_index(inplace=True)\n",
    "test.columns = ['Features', 'Target']\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "atmIIgNNFSll",
   "metadata": {
    "id": "atmIIgNNFSll"
   },
   "outputs": [],
   "source": [
    "train.Features = train['Features'].apply(clean_text)\n",
    "test.Features = test['Features'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8de55fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Features = train['Features'].apply(CleanText)\n",
    "test.Features = test['Features'].apply(CleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "85d526a1",
   "metadata": {
    "id": "85d526a1"
   },
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(train['Target'])\n",
    "# Drop column B as it is now encoded\n",
    "train = train.drop('Target',axis = 1)\n",
    "# Join the encoded df\n",
    "train = train.join(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "898f5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(test['Target'])\n",
    "# Drop column B as it is now encoded\n",
    "test = test.drop('Target',axis = 1)\n",
    "# Join the encoded df\n",
    "test = test.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cedb6f88",
   "metadata": {
    "id": "cedb6f88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "156ad054",
   "metadata": {
    "id": "156ad054"
   },
   "outputs": [],
   "source": [
    "counts = []\n",
    "categories = list(train.columns.values)\n",
    "# for i in categories:\n",
    "#     counts.append((i, df[i].sum()))\n",
    "# df_stats = pd.DataFrame(counts, columns=['category', 'number_of_comments'])\n",
    "# df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4c34614b",
   "metadata": {
    "id": "4c34614b"
   },
   "outputs": [],
   "source": [
    "# train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b60eb2e7",
   "metadata": {
    "id": "b60eb2e7"
   },
   "outputs": [],
   "source": [
    "X_train = train.Features\n",
    "X_test  = test.Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "617c4dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    from vmbkubvmsdccbuffaloedu neil b gandler sub...\n",
       "1    from rick miller subject xface organization ju...\n",
       "2    from mathew subject re strong weak atheism org...\n",
       "3    from bakkencsarizonaedu dave bakken subject re...\n",
       "4    from liveseysolntzewpdsgicom jon livesey subje...\n",
       "5    from banschbachvmsocomokstateedu subject re ca...\n",
       "6    from petchgvggvgtekcom chuck subject daily ver...\n",
       "7    from fortmannsuperbowlundacza paul fortmann pg...\n",
       "8    from kartikhlscom kartik chandrasekhar subject...\n",
       "9    from tmcspartanacbrockuca tim ciceran subject ...\n",
       "Name: Features, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b835ff95",
   "metadata": {
    "id": "b835ff95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : 1 \n",
      "For MultinomialNB() classifier accuracy on train data is 0.9636685866194062\n",
      "For MultinomialNB() classifier accuracy on test data is 0.8348868175765646\n",
      "\n",
      "\n",
      "Model : 2 \n",
      "For SGDClassifier(alpha=0.001, random_state=42) classifier accuracy on train data is 0.997341603898981\n",
      "For SGDClassifier(alpha=0.001, random_state=42) classifier accuracy on test data is 0.9114513981358189\n",
      "\n",
      "\n",
      "Model : 3 \n",
      "For MultinomialNB() classifier accuracy on train data is 0.9884802835622508\n",
      "For MultinomialNB() classifier accuracy on test data is 0.8894806924101198\n",
      "\n",
      "\n",
      "Model : 4 \n",
      "For OneVsRestClassifier(estimator=LinearSVC(), n_jobs=1) classifier accuracy on train data is 1.0\n",
      "For OneVsRestClassifier(estimator=LinearSVC(), n_jobs=1) classifier accuracy on test data is 0.9260985352862849\n",
      "\n",
      "\n",
      "Model : 5 \n",
      "For OneVsRestClassifier(estimator=LogisticRegression(solver='sag'), n_jobs=1) classifier accuracy on train data is 0.9924678777137793\n",
      "For OneVsRestClassifier(estimator=LogisticRegression(solver='sag'), n_jobs=1) classifier accuracy on test data is 0.9001331557922769\n",
      "\n",
      "\n",
      "Model : 6 \n",
      "For OneVsRestClassifier(estimator=MultinomialNB()) classifier accuracy on train data is 0.9889233495790872\n",
      "For OneVsRestClassifier(estimator=MultinomialNB()) classifier accuracy on test data is 0.8921438082556591\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "for classfier in classfiers:\n",
    "    count+=1\n",
    "    print(\"Model : {} \".format(count))\n",
    "    calc_accuracy_score(classfier, X_train , twenty_train.target, X_test, twenty_test.target)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "81c2cebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from vmbkubvmsdccbuffaloedu neil b gandler\\nsu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from rick miller  \\nsubject xface\\norganizatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from mathew  \\nsubject re strong  weak atheism...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>from bakkencsarizonaedu dave bakken\\nsubject r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from liveseysolntzewpdsgicom jon livesey\\nsubj...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7527</th>\n",
       "      <td>from richmondspiffprincetonedu stupendous man\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7528</th>\n",
       "      <td>from smytonjmurrallegedu jim smyton\\nsubject r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7529</th>\n",
       "      <td>from hhendersonvaxclarkuedu\\nsubject re game l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7530</th>\n",
       "      <td>from bzamutarlgutaedu \\nsubject intel chmos  d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7531</th>\n",
       "      <td>from adamsjgtewdmtvgtegsccom\\nsubject re homos...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7532 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Features  0  1  2  3  4  5  6  \\\n",
       "0     from vmbkubvmsdccbuffaloedu neil b gandler\\nsu...  0  0  0  0  0  0  0   \n",
       "1     from rick miller  \\nsubject xface\\norganizatio...  0  0  0  0  0  1  0   \n",
       "2     from mathew  \\nsubject re strong  weak atheism...  1  0  0  0  0  0  0   \n",
       "3     from bakkencsarizonaedu dave bakken\\nsubject r...  0  0  0  0  0  0  0   \n",
       "4     from liveseysolntzewpdsgicom jon livesey\\nsubj...  0  0  0  0  0  0  0   \n",
       "...                                                 ... .. .. .. .. .. .. ..   \n",
       "7527  from richmondspiffprincetonedu stupendous man\\...  0  0  0  0  0  0  0   \n",
       "7528  from smytonjmurrallegedu jim smyton\\nsubject r...  0  0  0  0  1  0  0   \n",
       "7529  from hhendersonvaxclarkuedu\\nsubject re game l...  0  0  0  0  0  0  0   \n",
       "7530  from bzamutarlgutaedu \\nsubject intel chmos  d...  0  0  0  0  0  0  1   \n",
       "7531  from adamsjgtewdmtvgtegsccom\\nsubject re homos...  0  0  0  0  0  0  0   \n",
       "\n",
       "      7  8  ...  10  11  12  13  14  15  16  17  18  19  \n",
       "0     1  0  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "1     0  0  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "2     0  0  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "3     0  0  ...   0   0   0   0   0   0   0   1   0   0  \n",
       "4     0  0  ...   0   0   0   0   0   0   0   0   0   1  \n",
       "...  .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  \n",
       "7527  0  0  ...   0   0   0   0   1   0   0   0   0   0  \n",
       "7528  0  0  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "7529  0  0  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "7530  0  0  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "7531  0  0  ...   0   0   0   0   0   1   0   0   0   0  \n",
       "\n",
       "[7532 rows x 21 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f8d02614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Features',\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0057c1cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0057c1cb",
    "outputId": "0eacc8c4-d479-4fff-feff-6224b61ef81a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing 0\n",
      "[0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Test accuracy is 0.9799522039298991\n",
      "... Processing 1\n",
      "[0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Test accuracy is 0.9731810939989378\n",
      "... Processing 2\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Test accuracy is 0.9709240573552841\n",
      "... Processing 3\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Test accuracy is 0.97052575677111\n"
     ]
    }
   ],
   "source": [
    "for category in categories[1:5]:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model using X_dtm & y\n",
    "    SVC_pipeline.fit(X_train, train[category])\n",
    "    # compute the testing accuracy\n",
    "    prediction = SVC_pipeline.predict(X_test)\n",
    "    print(SVC_pipeline.predict(X_test[:100]))\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94d59578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_pipeline.predict(np.array([X_test[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "083f7843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Features', 'Target']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    prediction = NB_pipeline.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(test_narrative.iloc[i][:50], \"...\")\n",
    "    print('Actual label:' + test_product.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m-XA-t1kGL75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-XA-t1kGL75",
    "outputId": "f767325c-2d20-4a19-b503-89d4044b4883"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : 1 \n",
      "For MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) classifier accuracy on train data is 0.9261213720316622\n",
      "For MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) classifier accuracy on test data is 0.8299410819496519\n",
      "\n",
      "\n",
      "Model : 2 \n",
      "For SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False) classifier accuracy on train data is 0.9740105540897097\n",
      "For SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False) classifier accuracy on test data is 0.8853776111408677\n",
      "\n",
      "\n",
      "Model : 3 \n",
      "For MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) classifier accuracy on train data is 0.9583113456464379\n",
      "For MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) classifier accuracy on test data is 0.8794858061060525\n",
      "\n",
      "\n",
      "Model : 4 \n",
      "For OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
      "                                        fit_intercept=True, intercept_scaling=1,\n",
      "                                        loss='squared_hinge', max_iter=1000,\n",
      "                                        multi_class='ovr', penalty='l2',\n",
      "                                        random_state=None, tol=0.0001,\n",
      "                                        verbose=0),\n",
      "                    n_jobs=1) classifier accuracy on train data is 0.9996042216358839\n",
      "For OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
      "                                        fit_intercept=True, intercept_scaling=1,\n",
      "                                        loss='squared_hinge', max_iter=1000,\n",
      "                                        multi_class='ovr', penalty='l2',\n",
      "                                        random_state=None, tol=0.0001,\n",
      "                                        verbose=0),\n",
      "                    n_jobs=1) classifier accuracy on test data is 0.9196572040707016\n",
      "\n",
      "\n",
      "Model : 5 \n",
      "For OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
      "                                                 dual=False, fit_intercept=True,\n",
      "                                                 intercept_scaling=1,\n",
      "                                                 l1_ratio=None, max_iter=100,\n",
      "                                                 multi_class='auto',\n",
      "                                                 n_jobs=None, penalty='l2',\n",
      "                                                 random_state=None,\n",
      "                                                 solver='sag', tol=0.0001,\n",
      "                                                 verbose=0, warm_start=False),\n",
      "                    n_jobs=1) classifier accuracy on train data is 0.974802110817942\n",
      "For OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
      "                                                 dual=False, fit_intercept=True,\n",
      "                                                 intercept_scaling=1,\n",
      "                                                 l1_ratio=None, max_iter=100,\n",
      "                                                 multi_class='auto',\n",
      "                                                 n_jobs=None, penalty='l2',\n",
      "                                                 random_state=None,\n",
      "                                                 solver='sag', tol=0.0001,\n",
      "                                                 verbose=0, warm_start=False),\n",
      "                    n_jobs=1) classifier accuracy on test data is 0.8947509373326191\n",
      "\n",
      "\n",
      "Model : 6 \n",
      "For OneVsRestClassifier(estimator=MultinomialNB(alpha=1.0, class_prior=None,\n",
      "                                            fit_prior=True),\n",
      "                    n_jobs=None) classifier accuracy on train data is 0.9658311345646438\n",
      "For OneVsRestClassifier(estimator=MultinomialNB(alpha=1.0, class_prior=None,\n",
      "                                            fit_prior=True),\n",
      "                    n_jobs=None) classifier accuracy on test data is 0.8899303695768612\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count =0\n",
    "for classfier in classfiers:\n",
    "    count+=1\n",
    "    print(\"Model : {} \".format(count))\n",
    "    calc_accuracy_score(classfier, X_train, y_train, X_test, y_test)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf19d5",
   "metadata": {
    "id": "9ecf19d5",
    "outputId": "6ba6d7a1-51fa-4c7d-f279-b499eb4fab34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf-svm__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "gs_clf.best_score_\n",
    "gs_clf.best_params_\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dab184",
   "metadata": {
    "id": "95dab184"
   },
   "outputs": [],
   "source": [
    "# count =0\n",
    "# for classfier in [gs_clf, gs_clf_svm]:\n",
    "#     count+=1\n",
    "#     print(\"Model : {} \".format(count))\n",
    "#     calc_accuracy_score(classfier, twenty_train.data, twenty_train.target, twenty_test.data, twenty_test.target, gs=False)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68615fb",
   "metadata": {
    "id": "e68615fb"
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"estimator__C\": [1,2,4,8],\n",
    "    \"estimator__kernel\": [\"poly\",\"rbf\"],\n",
    "    \"estimator__degree\":[1, 2, 3, 4],\n",
    "}\n",
    "gs_clf = GridSearchCV(SVC_pipeline, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f671a8",
   "metadata": {
    "id": "26f671a8",
    "outputId": "fa66fceb-f8d8-4b89-9237-0b41a600d2fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cv', 'error_score', 'estimator__memory', 'estimator__steps', 'estimator__verbose', 'estimator__tfidf', 'estimator__clf', 'estimator__tfidf__analyzer', 'estimator__tfidf__binary', 'estimator__tfidf__decode_error', 'estimator__tfidf__dtype', 'estimator__tfidf__encoding', 'estimator__tfidf__input', 'estimator__tfidf__lowercase', 'estimator__tfidf__max_df', 'estimator__tfidf__max_features', 'estimator__tfidf__min_df', 'estimator__tfidf__ngram_range', 'estimator__tfidf__norm', 'estimator__tfidf__preprocessor', 'estimator__tfidf__smooth_idf', 'estimator__tfidf__stop_words', 'estimator__tfidf__strip_accents', 'estimator__tfidf__sublinear_tf', 'estimator__tfidf__token_pattern', 'estimator__tfidf__tokenizer', 'estimator__tfidf__use_idf', 'estimator__tfidf__vocabulary', 'estimator__clf__estimator__C', 'estimator__clf__estimator__class_weight', 'estimator__clf__estimator__dual', 'estimator__clf__estimator__fit_intercept', 'estimator__clf__estimator__intercept_scaling', 'estimator__clf__estimator__loss', 'estimator__clf__estimator__max_iter', 'estimator__clf__estimator__multi_class', 'estimator__clf__estimator__penalty', 'estimator__clf__estimator__random_state', 'estimator__clf__estimator__tol', 'estimator__clf__estimator__verbose', 'estimator__clf__estimator', 'estimator__clf__n_jobs', 'estimator', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890bcf00",
   "metadata": {
    "id": "890bcf00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdfe41d",
   "metadata": {
    "id": "7fdfe41d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40ff5c",
   "metadata": {
    "id": "3d40ff5c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad243b78",
   "metadata": {
    "id": "ad243b78"
   },
   "outputs": [],
   "source": [
    "df['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cccb849",
   "metadata": {
    "id": "3cccb849"
   },
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "train_size = int(len(df) * .8)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(df) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea7e4e",
   "metadata": {
    "id": "f0ea7e4e"
   },
   "outputs": [],
   "source": [
    "train_narrative = twen\n",
    "train_product = df['Product'][:train_size]\n",
    "\n",
    "test_narrative = df['Consumer complaint narrative'][train_size:]\n",
    "test_product = df['Product'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd4734",
   "metadata": {
    "id": "71bd4734"
   },
   "outputs": [],
   "source": [
    "train_narrative = twenty_train.data\n",
    "train_product = twenty_train.target\n",
    "\n",
    "test_narrative = twenty_test.data\n",
    "test_product = twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b4362",
   "metadata": {
    "id": "cf3b4362"
   },
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_narrative) # only fit on train\n",
    "x_train = tokenize.texts_to_matrix(train_narrative)\n",
    "x_test = tokenize.texts_to_matrix(test_narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80272b1",
   "metadata": {
    "id": "a80272b1"
   },
   "outputs": [],
   "source": [
    "# Use sklearn utility to convert label strings to numbered index\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_product)\n",
    "y_train = encoder.transform(train_product)\n",
    "y_test = encoder.transform(test_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792ed52",
   "metadata": {
    "id": "8792ed52"
   },
   "outputs": [],
   "source": [
    "# Converts the labels to a one-hot representation\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08539210",
   "metadata": {
    "id": "08539210",
    "outputId": "2b6a7ce1-d71a-4be3-b1c2-f0d4344b6507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (11314, 1000)\n",
      "x_test shape: (7532, 1000)\n",
      "y_train shape: (11314, 20)\n",
      "y_test shape: (7532, 20)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dimenstions of our training and test data (this is helpful to debug)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24778874",
   "metadata": {
    "id": "24778874"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f8577",
   "metadata": {
    "id": "491f8577",
    "outputId": "b27ee937-aafe-44bc-d877-eb29150b77aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 23:34:45.057190: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483902c",
   "metadata": {
    "id": "5483902c",
    "outputId": "3f19ab46-1f65-4014-b938-39a3c3b3d270"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 23:34:49.597499: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "319/319 [==============================] - 19s 7ms/step - loss: 2.2704 - accuracy: 0.3393 - val_loss: 0.9573 - val_accuracy: 0.7314\n",
      "Epoch 2/5\n",
      "319/319 [==============================] - 1s 4ms/step - loss: 0.8193 - accuracy: 0.7710 - val_loss: 0.7684 - val_accuracy: 0.7721\n",
      "Epoch 3/5\n",
      "319/319 [==============================] - 1s 4ms/step - loss: 0.5417 - accuracy: 0.8453 - val_loss: 0.7313 - val_accuracy: 0.7836\n",
      "Epoch 4/5\n",
      "319/319 [==============================] - 1s 4ms/step - loss: 0.4035 - accuracy: 0.8877 - val_loss: 0.7382 - val_accuracy: 0.7792\n",
      "Epoch 5/5\n",
      "319/319 [==============================] - 1s 4ms/step - loss: 0.3181 - accuracy: 0.9146 - val_loss: 0.7332 - val_accuracy: 0.7959\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb955a",
   "metadata": {
    "id": "ecbb955a",
    "outputId": "a942a40d-3248-456a-8fda-e4aecee8809b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 1s 2ms/step - loss: 1.2025 - accuracy: 0.6668\n",
      "Test score: 1.2024712562561035\n",
      "Test accuracy: 0.666755199432373\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of our trained model\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd7531",
   "metadata": {
    "id": "e0cd7531"
   },
   "outputs": [],
   "source": [
    "# # Here's how to generate a prediction on individual examples\n",
    "# text_labels = encoder.classes_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17a1feb",
   "metadata": {
    "id": "d17a1feb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19f6aa",
   "metadata": {
    "id": "5b19f6aa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1435e",
   "metadata": {
    "id": "02c1435e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331916aa",
   "metadata": {
    "id": "331916aa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a6cfd",
   "metadata": {
    "id": "0d5a6cfd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc547d",
   "metadata": {
    "id": "07dc547d"
   },
   "outputs": [],
   "source": [
    "text = 'Hello 123   <br> hghjh</br> completed implementation dÃ¨Ã¨p lÃ¨arning Ã¡nd cÃ¶mputer vÃ­sÃ­Ã¶n satyam@gmail.com , p>>..>>>#9 YUJ hsddu gh ssd [[23/23/344]] https:www.google.com js'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826fb8fa",
   "metadata": {
    "id": "826fb8fa",
    "outputId": "6594e767-c62a-4e3a-ef58-8e65d486e0be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello      hghjh  completed implementation deep learning and computer vision satyamgmailcom  p yuj hsddu gh ssd   js'"
      ]
     },
     "execution_count": 121,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f801905c",
   "metadata": {
    "id": "f801905c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "classfication.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
